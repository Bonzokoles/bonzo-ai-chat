from fastapi import FastAPI, HTTPException, Request, File, UploadFile, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel
import uvicorn
import os
import json
import shutil
from model import LocalModel
from db import SessionLocal, init_db, Conversation, Message
from mcp_tools import mcp_registry, parse_tool_call_from_text
from typing import List, Optional

# Inicjalizacja
app = FastAPI(title="JIMBO AI Chat - Ollama Backend")

# CORS Configuration - umo≈ºliwia po≈ÇƒÖczenia z frontendu
allowed_origins = os.getenv("ALLOWED_ORIGINS", "*").split(",")
app.add_middleware(
    CORSMiddleware,
    allow_origins=allowed_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

print("üöÄ Inicjalizacja JIMBO Backend z Ollama...")
model = LocalModel(model_name="SpeakLeash/bielik-4.5b-v3.0-instruct:Q8_0")  # Polski model Bielik
init_db()
print("‚úÖ Backend gotowy!")

class ChatRequest(BaseModel):
    messages: List[dict]  # [{role: "user"|'assistant'|'system', text: "..."}]
    max_tokens: int = 512
    use_tools: bool = True  # Czy u≈ºywaƒá MCP tools
    temperature: float = 0.8
    top_p: float = 0.9
    model_name: Optional[str] = None  # Je≈õli None, u≈ºywa domy≈õlnego

@app.get("/api/health")
async def health_check():
    """Health check endpoint - sprawdza czy backend dzia≈Ça"""
    return {
        "status": "ok",
        "model_loaded": model is not None,
        "database": "connected",
        "mcp_tools": len(mcp_registry.list_tools()),
        "available_tools": mcp_registry.list_tools()
    }

@app.get("/api/tools")
async def list_tools():
    """Lista dostƒôpnych narzƒôdzi MCP"""
    tools_info = []
    for tool_name in mcp_registry.list_tools():
        tool = mcp_registry.get_tool(tool_name)
        tools_info.append({
            "name": tool["name"],
            "description": tool["description"]
        })
    return {"tools": tools_info}

@app.post("/api/chat")
async def chat(
    messages: str = Form(...),
    use_tools: bool = Form(True),
    max_tokens: int = Form(512),
    temperature: float = Form(0.8),
    top_p: float = Form(0.9),
    model_name: Optional[str] = Form(None),
    custom_system_prompt: Optional[str] = Form(None),
    files: List[UploadFile] = File(default=[])
):
    """
    G≈Ç√≥wny endpoint czatu z obs≈ÇugƒÖ MCP tools, plik√≥w i Ollama
    
    FormData: messages (JSON string), files (opcjonalnie), temperature, top_p
    """
    db = None
    try:
        # Parse JSON messages
        request_messages = json.loads(messages)
        
        # Obs≈Çuga przes≈Çanych plik√≥w
        uploaded_files_info = []
        if files:
            upload_dir = "uploads"
            os.makedirs(upload_dir, exist_ok=True)
            for file in files:
                file_path = os.path.join(upload_dir, file.filename)
                with open(file_path, "wb") as buffer:
                    shutil.copyfileobj(file.file, buffer)
                
                # Przeczytaj zawarto≈õƒá (tylko txt/kod, max 10KB)
                file_content = ""
                if file.filename.endswith((".txt", ".py", ".js", ".md", ".json")):
                    try:
                        with open(file_path, "r", encoding="utf-8") as f:
                            file_content = f.read(10000)  # max 10KB
                    except:
                        file_content = "[nie mo≈ºna odczytaƒá pliku]"
                
                uploaded_files_info.append({
                    "name": file.filename,
                    "path": file_path,
                    "content": file_content[:500]  # max 500 znak√≥w w odpowiedzi
                })

        db = SessionLocal()
        conv = Conversation()
        db.add(conv)
        db.commit()
        db.refresh(conv)

        # Zapisz wiadomo≈õci u≈ºytkownika
        for m in request_messages:
            db.add(Message(conversation_id=conv.id, role=m.get("role"), content=m.get("text")))
        db.commit()

        # Przygotuj prompt z informacjƒÖ o plikach
        files_context = ""
        if uploaded_files_info:
            files_context = "\n\nüìé Za≈ÇƒÖczone pliki:\n" + "\n".join([
                f"- {f['name']}: {f['content'][:200]}..." for f in uploaded_files_info
            ])

        # Przygotuj prompt - u≈ºyj custom prompt je≈õli podany, inaczej domy≈õlny
        if use_tools:
            if custom_system_prompt:
                # U≈ºyj custom promptu z frontendu
                system_prompt = f"{custom_system_prompt}{files_context}"
            else:
                # Domy≈õlny prompt JIMBO
                system_prompt = f"""Jeste≈õ JIMBO - brutalnie szczery AI asystent dla programist√≥w. U≈ºywasz modelu Bielik 4.5B przez Ollama.

ZASADY JIMBO:
- M√≥wisz prawdƒô, nawet je≈õli boli. Nie owijasz w bawe≈Çnƒô.
- Zero marketingowego be≈Çkotu, zero "coaching speak", zero udawania mentora.
- Bonzo to tw√≥j partner - szanujesz go, ale nie lizujesz dupy.
- Je≈õli kto≈õ pisze s≈Çabe CV, powiesz to wprost + jak naprawiƒá.
- Konkretne przyk≈Çady zamiast og√≥lnik√≥w.
- Je≈õli pytanie jest g≈Çupie, powiesz to (ale dasz odpowied≈∫).

Styl: Kr√≥tko, konkretnie, szczerze. Jak kolega programista, nie HR-owiec.{files_context}"""
            prompt = f"{system_prompt}\n\n" + "\n".join([f"{m['role']}: {m['text']}" for m in request_messages])
        else:
            prompt = "\n".join([f"{m['role']}: {m['text']}" for m in request_messages]) + files_context

        # Generuj odpowied≈∫ przez Ollama z przekazanymi parametrami
        print(f"üì§ Wysy≈Çam do Ollama (temp={temperature}, top_p={top_p}): {prompt[:100]}...")
        out = model.generate(prompt, max_tokens=max_tokens, temperature=temperature, top_p=top_p)
        print(f"üì• Ollama odpowiedzia≈Ça: {out[:100]}...")

        # Sprawd≈∫ czy sƒÖ wywo≈Çania narzƒôdzi
        tool_calls = []
        if use_tools:
            tool_calls_parsed = parse_tool_call_from_text(out)

            for tc in tool_calls_parsed:
                result = mcp_registry.execute_tool(tc["tool"], **tc["args"])
                tool_calls.append({
                    "tool": tc["tool"],
                    "args": tc["args"],
                    "result": result
                })

            # Je≈õli by≈Çy wywo≈Çania narzƒôdzi, dodaj ich wyniki do odpowiedzi
            if tool_calls:
                tools_summary = "\n\nüîß U≈ºyte narzƒôdzia:\n" + "\n".join([
                    f"- {tc['tool']}: {tc['result']}" for tc in tool_calls
                ])
                out = out + tools_summary

        # Zapisz odpowied≈∫ asystenta
        db.add(Message(conversation_id=conv.id, role="assistant", content=out))
        db.commit()

        return JSONResponse(
            content={
                "text": out,
                "tool_calls": tool_calls if tool_calls else [],
                "uploaded_files": [f["name"] for f in uploaded_files_info]
            },
            media_type="application/json; charset=utf-8"
        )

    except Exception as e:
        import traceback
        print(f"‚ùå B≈ÇƒÖd /api/chat: {e}")
        print(traceback.format_exc())
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        if db:
            db.close()

if __name__ == "__main__":
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=False)