# Wrapper do Ollama API - lokalny LLM bez GPU requirements
import requests
import os

class LocalModel:
    def __init__(self, model_name="llama3.2:latest", ollama_url=None):
        """
        Integracja z Ollama - wymaga uruchomionego Ollama serwera
        Instalacja: https://ollama.com/download
        Pobierz model: ollama pull llama3.2
        """
        self.model_name = model_name
        self.ollama_url = ollama_url or os.getenv("OLLAMA_URL", "http://localhost:11434")
        
        # Sprawd≈∫ czy Ollama dzia≈Ça
        try:
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=2)
            if response.status_code == 200:
                models = response.json().get("models", [])
                print(f"‚úÖ Ollama po≈ÇƒÖczona - dostƒôpne modele: {[m['name'] for m in models]}")
                
                # Je≈õli model nie istnieje, u≈ºyj pierwszego dostƒôpnego
                if models and not any(m['name'].startswith(model_name.split(':')[0]) for m in models):
                    self.model_name = models[0]['name']
                    print(f"‚ö†Ô∏è Model {model_name} nie znaleziony, u≈ºywam {self.model_name}")
            else:
                print(f"‚ö†Ô∏è Ollama odpowiada ale status: {response.status_code}")
        except Exception as e:
            print(f"‚ùå Ollama niedostƒôpna: {e}")
            print("üí° Uruchom: ollama serve")
            print("üí° Pobierz model: ollama pull llama3.2")
    
    def generate(self, prompt: str, max_tokens: int = 512, temperature: float = 0.7, top_p: float = 0.9):
        """Generuj tekst u≈ºywajƒÖc Ollama API"""
        try:
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json={
                    "model": self.model_name,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "num_predict": max_tokens,
                        "temperature": temperature,
                        "top_p": top_p
                    }
                },
                timeout=60
            )
            
            if response.status_code == 200:
                return response.json().get("response", "Brak odpowiedzi z modelu")
            else:
                return f"‚ùå B≈ÇƒÖd Ollama: {response.status_code} - {response.text}"
                
        except requests.exceptions.ConnectionError:
            return "‚ùå Nie mogƒô po≈ÇƒÖczyƒá z Ollama. Uruchom: ollama serve"
        except Exception as e:
            return f"‚ùå B≈ÇƒÖd generowania: {str(e)}"